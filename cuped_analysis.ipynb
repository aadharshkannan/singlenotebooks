{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CUPED Comparison: Previous vs Next Period\n",
        "\n",
        "**Objectives:** Compare outcomes between a previous period and a next period using CUPED variance reduction while treating each `LLMScore` metric independently. The workflow builds the CUPED baseline from the previous period only, applies the adjustment to both periods, and analyzes per-case and per-metric effects with bootstrap uncertainty quantification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1 \u2014 Configuration & Library Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from typing import Dict\n",
        "from IPython.display import display\n",
        "\n",
        "try:\n",
        "    from statsmodels.api import OLS, add_constant\n",
        "except ModuleNotFoundError:\n",
        "    OLS = None\n",
        "    add_constant = None\n",
        "\n",
        "# Plot settings\n",
        "sns.set_theme(style='whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "# Configuration\n",
        "PREVIOUS_PATH = Path('data/previous.parquet')\n",
        "NEXT_PATH = Path('data/next.parquet')\n",
        "N_BOOT = 10_000\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "print('Configuration loaded:')\n",
        "print(f\"Previous path: {PREVIOUS_PATH}\")\n",
        "print(f\"Next path: {NEXT_PATH}\")\n",
        "print(f\"Bootstrap iterations: {N_BOOT}\")\n",
        "print(f\"Random seed: {RANDOM_SEED}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2 \u2014 Robust File Loaders\n",
        "REQUIRED_COLUMNS = ['RunID', 'TrialID', 'TestCaseID', 'LLMScore', 'Value']\n",
        "\n",
        "def load_dataset(path: Path) -> pd.DataFrame:\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"Input file not found: {path}\")\n",
        "    if path.suffix.lower() == '.csv':\n",
        "        df = pd.read_csv(path)\n",
        "    elif path.suffix.lower() in {'.parquet', '.pq'}:\n",
        "        df = pd.read_parquet(path)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file type for {path}. Use CSV or Parquet.\")\n",
        "    missing_cols = set(REQUIRED_COLUMNS) - set(df.columns)\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"Dataset {path} is missing required columns: {sorted(missing_cols)}\")\n",
        "    # Drop TrialID explicitly as instructed (single TrialID only).\n",
        "    df = df.drop(columns=['TrialID'])\n",
        "    df['Value'] = pd.to_numeric(df['Value'], errors='coerce')\n",
        "    if df['Value'].isna().any():\n",
        "        bad_rows = df[df['Value'].isna()]\n",
        "        raise ValueError(\n",
        "            f\"Non-numeric values detected in 'Value' column for dataset {path}. \"\n",
        "            f\"Offending row indices (first 5): {bad_rows.index.tolist()[:5]}\"\n",
        "        )\n",
        "    return df\n",
        "\n",
        "previous_df = load_dataset(PREVIOUS_PATH)\n",
        "next_df = load_dataset(NEXT_PATH)\n",
        "print('Loaded previous:', previous_df.shape)\n",
        "print('Loaded next:', next_df.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3 \u2014 Data Hygiene & Coverage Snapshot\n",
        "\n",
        "def coverage_snapshot(prev: pd.DataFrame, nxt: pd.DataFrame) -> pd.DataFrame:\n",
        "    summary_rows = []\n",
        "    metrics = sorted(set(prev['LLMScore']).union(nxt['LLMScore']))\n",
        "    for metric in metrics:\n",
        "        prev_cases = set(prev.loc[prev['LLMScore'] == metric, 'TestCaseID'])\n",
        "        next_cases = set(nxt.loc[nxt['LLMScore'] == metric, 'TestCaseID'])\n",
        "        inter = prev_cases & next_cases\n",
        "        coverage = len(inter) / len(next_cases) if next_cases else np.nan\n",
        "        summary_rows.append({\n",
        "            'LLMScore': metric,\n",
        "            'cases_prev': len(prev_cases),\n",
        "            'cases_next': len(next_cases),\n",
        "            'cases_intersection': len(inter),\n",
        "            'coverage_rate_next': coverage,\n",
        "            'low_overlap_flag': (not np.isnan(coverage)) and (coverage < 0.7)\n",
        "        })\n",
        "    return pd.DataFrame(summary_rows)\n",
        "\n",
        "prev_counts = previous_df.groupby('LLMScore').agg(\n",
        "    rows=('TestCaseID', 'size'),\n",
        "    unique_cases=('TestCaseID', pd.Series.nunique)\n",
        ").reset_index()\n",
        "next_counts = next_df.groupby('LLMScore').agg(\n",
        "    rows=('TestCaseID', 'size'),\n",
        "    unique_cases=('TestCaseID', pd.Series.nunique)\n",
        ").reset_index()\n",
        "print('Previous snapshot:')\n",
        "display(prev_counts)\n",
        "print('Next snapshot:')\n",
        "display(next_counts)\n",
        "coverage_df = coverage_snapshot(previous_df, next_df)\n",
        "print('Coverage overview:')\n",
        "display(coverage_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4 \u2014 Build CUPED Baseline (X) from Previous\n",
        "X_baseline = (\n",
        "    previous_df\n",
        "    .groupby(['TestCaseID', 'LLMScore'], as_index=False)['Value']\n",
        "    .mean()\n",
        "    .rename(columns={'Value': 'X_baseline'})\n",
        ")\n",
        "metric_baseline_stats = (\n",
        "    X_baseline.groupby('LLMScore')['X_baseline']\n",
        "    .agg(['mean', 'std', 'count'])\n",
        "    .rename(columns={'count': 'n_cases'})\n",
        "    .reset_index()\n",
        ")\n",
        "print('Baseline preview:')\n",
        "display(X_baseline.head())\n",
        "print('Baseline statistics per metric:')\n",
        "display(metric_baseline_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5 \u2014 Estimate \u03b8 per Metric (CUPED coefficient)\n",
        "prev_with_baseline = previous_df.merge(X_baseline, on=['TestCaseID', 'LLMScore'], how='left')\n",
        "theta_rows = []\n",
        "for metric, group in prev_with_baseline.groupby('LLMScore'):\n",
        "    x = group['X_baseline']\n",
        "    y = group['Value']\n",
        "    x_centered = x - x.mean()\n",
        "    y_centered = y - y.mean()\n",
        "    var_x = x_centered.var(ddof=0)\n",
        "    if np.isclose(var_x, 0):\n",
        "        theta = 0.0\n",
        "        warn = True\n",
        "    else:\n",
        "        cov = np.mean(x_centered * y_centered)\n",
        "        theta = cov / var_x\n",
        "        warn = False\n",
        "    corr_xy = np.corrcoef(x, y)[0, 1] if len(group) > 1 else np.nan\n",
        "    ols_slope = np.nan\n",
        "    if OLS is not None and not np.isclose(var_x, 0):\n",
        "        X = add_constant(x_centered)\n",
        "        model = OLS(y_centered, X).fit()\n",
        "        ols_slope = model.params[1]\n",
        "    theta_rows.append({\n",
        "        'LLMScore': metric,\n",
        "        'theta': theta,\n",
        "        'corr_XY': corr_xy,\n",
        "        'var_X': var_x,\n",
        "        'N_rows_prev': len(group),\n",
        "        'OLS_slope': ols_slope,\n",
        "        'variance_or_corr_flag': warn or (not np.isnan(corr_xy) and abs(corr_xy) < 0.05)\n",
        "    })\n",
        "\n",
        "theta_df = pd.DataFrame(theta_rows)\n",
        "print('Theta estimates per metric:')\n",
        "display(theta_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6 \u2014 Apply CUPED to Previous & Next\n",
        "mean_X_baseline = X_baseline.groupby('LLMScore')['X_baseline'].mean().rename('mean_X_baseline')\n",
        "X_with_mean = X_baseline.merge(mean_X_baseline, on='LLMScore', how='left')\n",
        "\n",
        "def apply_cuped(df: pd.DataFrame, label: str) -> pd.DataFrame:\n",
        "    merged = df.merge(X_with_mean, on=['TestCaseID', 'LLMScore'], how='left')\n",
        "    merged = merged.merge(theta_df[['LLMScore', 'theta']], on='LLMScore', how='left')\n",
        "    merged['X_baseline'] = merged['X_baseline'].fillna(merged['mean_X_baseline'])\n",
        "    merged['theta'] = merged['theta'].fillna(0.0)\n",
        "    merged['Value_CUPED'] = merged['Value'] - merged['theta'] * (merged['X_baseline'] - merged['mean_X_baseline'])\n",
        "    merged['Period'] = label\n",
        "    return merged\n",
        "\n",
        "previous_cuped = apply_cuped(previous_df, 'Previous')\n",
        "next_cuped = apply_cuped(next_df, 'Next')\n",
        "\n",
        "variance_rows = []\n",
        "for metric, group in pd.concat([previous_cuped, next_cuped]).groupby('LLMScore'):\n",
        "    var_raw = group['Value'].var(ddof=0)\n",
        "    var_cuped = group['Value_CUPED'].var(ddof=0)\n",
        "    reduction = 1 - (var_cuped / var_raw) if var_raw > 0 else np.nan\n",
        "    variance_rows.append({\n",
        "        'LLMScore': metric,\n",
        "        'var_raw': var_raw,\n",
        "        'var_cuped': var_cuped,\n",
        "        'variance_reduction_pct': reduction * 100 if not np.isnan(reduction) else np.nan\n",
        "    })\n",
        "variance_df = pd.DataFrame(variance_rows)\n",
        "print('Variance diagnostics per metric:')\n",
        "display(variance_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7 \u2014 Per-Case & Per-Metric Comparisons (CUPED space)\n",
        "all_cuped = pd.concat([previous_cuped, next_cuped], ignore_index=True)\n",
        "per_case = (\n",
        "    all_cuped\n",
        "    .groupby(['TestCaseID', 'LLMScore', 'Period'])['Value_CUPED']\n",
        "    .mean()\n",
        "    .unstack('Period')\n",
        "    .rename(columns={'Previous': 'Prev_CUPED_Mean', 'Next': 'Next_CUPED_Mean'})\n",
        ")\n",
        "per_case['Diff'] = per_case['Next_CUPED_Mean'] - per_case['Prev_CUPED_Mean']\n",
        "per_case = per_case.reset_index()\n",
        "print('Per-case CUPED summary:')\n",
        "display(per_case.head())\n",
        "\n",
        "def cohen_d(diff_values: np.ndarray) -> float:\n",
        "    diff_values = diff_values[~np.isnan(diff_values)]\n",
        "    if diff_values.size == 0:\n",
        "        return np.nan\n",
        "    mean_diff = diff_values.mean()\n",
        "    std_diff = diff_values.std(ddof=1)\n",
        "    return mean_diff / std_diff if std_diff > 0 else np.nan\n",
        "\n",
        "metric_summary_rows = []\n",
        "for metric, group in per_case.groupby('LLMScore'):\n",
        "    diffs = group['Diff'].dropna().values\n",
        "    prev_mean = group['Prev_CUPED_Mean'].mean()\n",
        "    next_mean = group['Next_CUPED_Mean'].mean()\n",
        "    percent_change = (next_mean / prev_mean - 1) if prev_mean != 0 else np.nan\n",
        "    share_improved = np.mean(diffs > 0) if len(diffs) else np.nan\n",
        "    metric_summary_rows.append({\n",
        "        'LLMScore': metric,\n",
        "        'mean_diff': np.mean(diffs) if len(diffs) else np.nan,\n",
        "        'cohens_d': cohen_d(diffs),\n",
        "        'percent_change': percent_change,\n",
        "        'share_improved': share_improved,\n",
        "        'n_cases': len(diffs)\n",
        "    })\n",
        "metric_summary_df = pd.DataFrame(metric_summary_rows)\n",
        "print('Per-metric CUPED summary:')\n",
        "display(metric_summary_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8 \u2014 Bootstrap Inference (clustered by TestCaseID)\n",
        "from numpy.random import default_rng\n",
        "rng = default_rng(RANDOM_SEED)\n",
        "\n",
        "def cluster_bootstrap_mean(diffs: pd.Series, ids: pd.Series, n_boot: int) -> np.ndarray:\n",
        "    unique_ids = ids.unique()\n",
        "    diff_map = pd.Series(diffs.values, index=ids.values).to_dict()\n",
        "    boot_means = np.empty(n_boot)\n",
        "    for i in range(n_boot):\n",
        "        sampled_ids = rng.choice(unique_ids, size=len(unique_ids), replace=True)\n",
        "        sampled_values = np.array([diff_map[idx] for idx in sampled_ids])\n",
        "        boot_means[i] = sampled_values.mean()\n",
        "    return boot_means\n",
        "\n",
        "bootstrap_rows = []\n",
        "bootstrap_results: Dict[str, np.ndarray] = {}\n",
        "for metric, group in per_case.groupby('LLMScore'):\n",
        "    mask = group['Diff'].notna()\n",
        "    if not mask.any():\n",
        "        continue\n",
        "    diffs = group.loc[mask, 'Diff']\n",
        "    ids = group.loc[mask, 'TestCaseID']\n",
        "    boot_means = cluster_bootstrap_mean(diffs, ids, N_BOOT)\n",
        "    bootstrap_results[metric] = boot_means\n",
        "    se = boot_means.std(ddof=1)\n",
        "    ci_lo, ci_hi = np.percentile(boot_means, [2.5, 97.5])\n",
        "    bootstrap_rows.append({\n",
        "        'LLMScore': metric,\n",
        "        'mean_diff': diffs.mean(),\n",
        "        'SE': se,\n",
        "        'CI_lo_95': ci_lo,\n",
        "        'CI_hi_95': ci_hi,\n",
        "        'N_cases': len(diffs)\n",
        "    })\n",
        "bootstrap_df = pd.DataFrame(bootstrap_rows)\n",
        "print('Bootstrap summary per metric:')\n",
        "display(bootstrap_df)\n",
        "\n",
        "pooled_diffs = per_case[['TestCaseID', 'LLMScore', 'Diff']].dropna()\n",
        "if not pooled_diffs.empty:\n",
        "    pooled_boot_means = []\n",
        "    unique_pairs = pooled_diffs[['TestCaseID', 'LLMScore']].apply(lambda x: tuple(x), axis=1).values\n",
        "    diff_map = {key: val for key, val in zip(unique_pairs, pooled_diffs['Diff'].values)}\n",
        "    for _ in range(N_BOOT):\n",
        "        sampled_pairs = rng.choice(unique_pairs, size=len(unique_pairs), replace=True)\n",
        "        pooled_boot_means.append(np.mean([diff_map[key] for key in sampled_pairs]))\n",
        "    print('Overall pooled bootstrap mean diff (descriptive):')\n",
        "    print(pd.Series(pooled_boot_means).describe(percentiles=[0.025, 0.5, 0.975]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9 \u2014 Visuals: Distribution & Effect Views\n",
        "plot_dir = Path('outputs/plots')\n",
        "plot_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def plot_metric_kdes(metric: str):\n",
        "    metric_data = all_cuped[all_cuped['LLMScore'] == metric]\n",
        "    plt.figure()\n",
        "    sns.kdeplot(data=metric_data, x='Value_CUPED', hue='Period', common_norm=False)\n",
        "    plt.title(f'CUPED Value Distribution \u2014 {metric}')\n",
        "    plt.xlabel('Value (CUPED)')\n",
        "    plt.ylabel('Density')\n",
        "    plt.tight_layout()\n",
        "    path = plot_dir / f\"kde_cuped_{metric}.png\"\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n",
        "    return path\n",
        "\n",
        "def plot_bootstrap_kde(metric: str, boot_means: np.ndarray):\n",
        "    plt.figure()\n",
        "    sns.kdeplot(boot_means, fill=True)\n",
        "    plt.axvline(0, color='black', linestyle='--', label='Zero')\n",
        "    plt.axvline(np.mean(boot_means), color='blue', linestyle='-', label='Mean')\n",
        "    ci_lo, ci_hi = np.percentile(boot_means, [2.5, 97.5])\n",
        "    plt.axvline(ci_lo, color='red', linestyle='--', label='95% CI')\n",
        "    plt.axvline(ci_hi, color='red', linestyle='--')\n",
        "    plt.title(f'Bootstrap Mean Diff Distribution \u2014 {metric}')\n",
        "    plt.xlabel('Bootstrap Mean Diff')\n",
        "    plt.ylabel('Density')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    path = plot_dir / f\"kde_bootstrap_{metric}.png\"\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n",
        "    return path\n",
        "\n",
        "metric_plot_paths = {}\n",
        "for metric in all_cuped['LLMScore'].unique():\n",
        "    metric_plot_paths[metric] = {'cuped_kde': plot_metric_kdes(metric)}\n",
        "    if metric in bootstrap_results:\n",
        "        metric_plot_paths[metric]['bootstrap_kde'] = plot_bootstrap_kde(metric, bootstrap_results[metric])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(max(8, 2 * max(1, len(bootstrap_df))), 6))\n",
        "bar_plot = sns.barplot(data=bootstrap_df, x='LLMScore', y='mean_diff', palette='viridis', ax=ax)\n",
        "positions = ax.get_xticks()\n",
        "ax.errorbar(positions, bootstrap_df['mean_diff'],\n",
        "            yerr=[bootstrap_df['mean_diff'] - bootstrap_df['CI_lo_95'],\n",
        "                  bootstrap_df['CI_hi_95'] - bootstrap_df['mean_diff']],\n",
        "            fmt='none', c='black', capsize=5)\n",
        "ax.axhline(0, color='black', linestyle='--')\n",
        "ax.set_title('Mean CUPED Difference per Metric with 95% CI')\n",
        "ax.set_xlabel('LLMScore')\n",
        "ax.set_ylabel('Mean Diff (Next - Previous)')\n",
        "fig.tight_layout()\n",
        "bar_plot_path = plot_dir / 'mean_diff_bar.png'\n",
        "fig.savefig(bar_plot_path)\n",
        "plt.close(fig)\n",
        "\n",
        "print('Plot files saved:')\n",
        "for metric, paths in metric_plot_paths.items():\n",
        "    for kind, path in paths.items():\n",
        "        print(metric, kind, path)\n",
        "print('Bar chart path:', bar_plot_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10 \u2014 Multiple-Comparison Context (optional)\n",
        "try:\n",
        "    from statsmodels.stats.multitest import multipletests\n",
        "    from scipy.stats import t\n",
        "except ImportError:\n",
        "    multipletests = None\n",
        "    t = None\n",
        "\n",
        "if multipletests is not None and t is not None and not per_case.empty:\n",
        "    p_values = []\n",
        "    for metric, group in per_case.groupby('LLMScore'):\n",
        "        diffs = group['Diff'].dropna()\n",
        "        if len(diffs) < 2:\n",
        "            p_values.append(np.nan)\n",
        "            continue\n",
        "        mean_diff = diffs.mean()\n",
        "        std_diff = diffs.std(ddof=1)\n",
        "        if std_diff == 0:\n",
        "            p_values.append(np.nan)\n",
        "            continue\n",
        "        t_stat = mean_diff / (std_diff / np.sqrt(len(diffs)))\n",
        "        p_val = 2 * (1 - t.cdf(abs(t_stat), df=len(diffs) - 1))\n",
        "        p_values.append(p_val)\n",
        "    temp_df = bootstrap_df.copy()\n",
        "    temp_df['p_value_ttest'] = p_values\n",
        "    valid_mask = temp_df['p_value_ttest'].notna()\n",
        "    if valid_mask.any():\n",
        "        _, q_values, _, _ = multipletests(temp_df.loc[valid_mask, 'p_value_ttest'], method='fdr_bh')\n",
        "        temp_df.loc[valid_mask, 'q_value_bh'] = q_values\n",
        "    print('Multiple comparison table (exploratory):')\n",
        "    display(temp_df)\n",
        "else:\n",
        "    print('Multiple comparison analysis skipped (dependencies unavailable or insufficient data).')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 11 \u2014 Sensitivity Analyses\n",
        "intersection_cases = set(previous_df['TestCaseID']).intersection(set(next_df['TestCaseID']))\n",
        "per_case_intersection = per_case[per_case['TestCaseID'].isin(intersection_cases)].copy()\n",
        "intersection_summary = (\n",
        "    per_case_intersection.groupby('LLMScore')['Diff']\n",
        "    .agg(mean_diff_intersection='mean', n_cases_intersection='count')\n",
        ")\n",
        "\n",
        "def winsorize(series: pd.Series, lower=0.025, upper=0.975) -> pd.Series:\n",
        "    if series.isna().all():\n",
        "        return series\n",
        "    lower_bound = series.quantile(lower)\n",
        "    upper_bound = series.quantile(upper)\n",
        "    return series.clip(lower_bound, upper_bound)\n",
        "\n",
        "per_case_winsor = per_case.copy()\n",
        "per_case_winsor['Diff_winsor'] = per_case.groupby('LLMScore')['Diff'].transform(winsorize)\n",
        "winsor_summary = (\n",
        "    per_case_winsor.groupby('LLMScore')['Diff_winsor']\n",
        "    .agg(mean_diff_winsor='mean')\n",
        ")\n",
        "median_summary = (\n",
        "    per_case.groupby('LLMScore')['Diff']\n",
        "    .agg(median_diff='median')\n",
        ")\n",
        "\n",
        "sensitivity_df = (\n",
        "    metric_summary_df[['LLMScore', 'mean_diff', 'n_cases']]\n",
        "    .merge(intersection_summary, on='LLMScore', how='left')\n",
        "    .merge(winsor_summary, on='LLMScore', how='left')\n",
        "    .merge(median_summary, on='LLMScore', how='left')\n",
        ")\n",
        "print('Sensitivity analyses overview:')\n",
        "display(sensitivity_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 12 \u2014 Final Summary Tables & Exports\n",
        "output_dir = Path('outputs')\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "per_case_path = output_dir / 'per_case_cuped_diff.csv'\n",
        "metric_summary_path = output_dir / 'metric_cuped_summary_raw.csv'\n",
        "bootstrap_path = output_dir / 'metric_cuped_summary_bootstrap.csv'\n",
        "theta_path = output_dir / 'theta_table.csv'\n",
        "variance_path = output_dir / 'variance_reduction_by_metric.csv'\n",
        "coverage_path = output_dir / 'coverage_by_metric.csv'\n",
        "\n",
        "per_case.to_csv(per_case_path, index=False)\n",
        "metric_summary_df.to_csv(metric_summary_path, index=False)\n",
        "bootstrap_df.to_csv(bootstrap_path, index=False)\n",
        "theta_df.to_csv(theta_path, index=False)\n",
        "variance_df.to_csv(variance_path, index=False)\n",
        "coverage_df.to_csv(coverage_path, index=False)\n",
        "\n",
        "print('Exports saved:')\n",
        "for path in [per_case_path, metric_summary_path, bootstrap_path, theta_path, variance_path, coverage_path]:\n",
        "    print(path)\n",
        "print('Plots directory:', plot_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpretation Guide\n",
        "\n",
        "* **Primary metric:** The CUPED-adjusted mean difference (Next \u2212 Previous) summarizes the directional effect for each `LLMScore`.\n",
        "* **Uncertainty:** 95% bootstrap confidence intervals that exclude zero indicate stronger evidence that the effect is non-zero.\n",
        "* **Variance reduction:** Compare raw vs CUPED variance; larger reductions imply a more informative baseline (`corr(X, Y)` close to \u00b11).\n",
        "* **Coverage:** Low overlap between previous and next test cases weakens comparability\u2014consult the intersection-only sensitivity table.\n",
        "* **Multiple metrics:** Use the exploratory q-values to control for multiple comparisons before declaring broad improvements.\n",
        "* **Practical impact:** Review Cohen\u2019s d, percent change, and share of improved cases to gauge real-world significance.\n",
        "* **Diagnostics:** Negative variance reduction or negligible correlations highlight metrics where CUPED may not help."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}