{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffabfce0",
   "metadata": {},
   "source": [
    "# CUPED Comparison: Previous vs Next Period\n",
    "\n",
    "**Objectives:** Compare outcomes between a previous period and a next period using CUPED variance reduction while treating each `LLMScore` metric independently. The workflow builds the CUPED baseline from the previous period only, applies the adjustment to both periods, and analyzes per-case and per-metric effects with bootstrap uncertainty quantification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d566d52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "Previous path: data\\UnchangedEvalRuns.csv\n",
      "Next path: data\\FakeEvalChangeOnlyOne.csv\n",
      "Bootstrap iterations: 10000\n",
      "Random seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — Configuration & Library Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "from IPython.display import display\n",
    "\n",
    "try:\n",
    "    from statsmodels.api import OLS, add_constant\n",
    "except ModuleNotFoundError:\n",
    "    OLS = None\n",
    "    add_constant = None\n",
    "\n",
    "# Plot settings\n",
    "sns.set_theme(style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Configuration\n",
    "PREVIOUS_PATH = Path('data/UnchangedEvalRuns.csv')\n",
    "NEXT_PATH = Path('data/FakeEvalChangeOnlyOne.csv')\n",
    "N_BOOT = 10_000\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print('Configuration loaded:')\n",
    "print(f\"Previous path: {PREVIOUS_PATH}\")\n",
    "print(f\"Next path: {NEXT_PATH}\")\n",
    "print(f\"Bootstrap iterations: {N_BOOT}\")\n",
    "print(f\"Random seed: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "088ad2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded previous: (1566, 4)\n",
      "Loaded next: (261, 4)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — Robust File Loaders\n",
    "REQUIRED_COLUMNS = ['RunId', 'TrialId', 'TestCaseId', 'LLMScore', 'Value']\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Input file not found: {path}\")\n",
    "    if path.suffix.lower() == '.csv':\n",
    "        df = pd.read_csv(path)\n",
    "    elif path.suffix.lower() in {'.parquet', '.pq'}:\n",
    "        df = pd.read_parquet(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type for {path}. Use CSV or Parquet.\")\n",
    "    missing_cols = set(REQUIRED_COLUMNS) - set(df.columns)\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Dataset {path} is missing required columns: {sorted(missing_cols)}\")\n",
    "    # Drop TrialID explicitly as instructed (single TrialID only).\n",
    "    df = df.drop(columns=['TrialId'])\n",
    "    df['Value'] = pd.to_numeric(df['Value'], errors='coerce')\n",
    "    if df['Value'].isna().any():\n",
    "        bad_rows = df[df['Value'].isna()]\n",
    "        raise ValueError(\n",
    "            f\"Non-numeric values detected in 'Value' column for dataset {path}. \"\n",
    "            f\"Offending row indices (first 5): {bad_rows.index.tolist()[:5]}\"\n",
    "        )\n",
    "    return df\n",
    "\n",
    "previous_df = load_dataset(PREVIOUS_PATH)\n",
    "next_df = load_dataset(NEXT_PATH)\n",
    "print('Loaded previous:', previous_df.shape)\n",
    "print('Loaded next:', next_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e5ec50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous snapshot:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LLMScore</th>\n",
       "      <th>rows</th>\n",
       "      <th>unique_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>522</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Efficiency</td>\n",
       "      <td>522</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Relevance</td>\n",
       "      <td>522</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     LLMScore  rows  unique_cases\n",
       "0    Accuracy   522            87\n",
       "1  Efficiency   522            87\n",
       "2   Relevance   522            87"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next snapshot:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LLMScore</th>\n",
       "      <th>rows</th>\n",
       "      <th>unique_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Efficiency</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Relevance</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     LLMScore  rows  unique_cases\n",
       "0    Accuracy    87            87\n",
       "1  Efficiency    87            87\n",
       "2   Relevance    87            87"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage overview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LLMScore</th>\n",
       "      <th>cases_prev</th>\n",
       "      <th>cases_next</th>\n",
       "      <th>cases_intersection</th>\n",
       "      <th>coverage_rate_next</th>\n",
       "      <th>low_overlap_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Efficiency</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Relevance</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     LLMScore  cases_prev  cases_next  cases_intersection  coverage_rate_next  \\\n",
       "0    Accuracy          87          87                  87                 1.0   \n",
       "1  Efficiency          87          87                  87                 1.0   \n",
       "2   Relevance          87          87                  87                 1.0   \n",
       "\n",
       "   low_overlap_flag  \n",
       "0             False  \n",
       "1             False  \n",
       "2             False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 3 — Data Hygiene & Coverage Snapshot\n",
    "\n",
    "def coverage_snapshot(prev: pd.DataFrame, nxt: pd.DataFrame) -> pd.DataFrame:\n",
    "    summary_rows = []\n",
    "    metrics = sorted(set(prev['LLMScore']).union(nxt['LLMScore']))\n",
    "    for metric in metrics:\n",
    "        prev_cases = set(prev.loc[prev['LLMScore'] == metric, 'TestCaseId'])\n",
    "        next_cases = set(nxt.loc[nxt['LLMScore'] == metric, 'TestCaseId'])\n",
    "        inter = prev_cases & next_cases\n",
    "        coverage = len(inter) / len(next_cases) if next_cases else np.nan\n",
    "        summary_rows.append({\n",
    "            'LLMScore': metric,\n",
    "            'cases_prev': len(prev_cases),\n",
    "            'cases_next': len(next_cases),\n",
    "            'cases_intersection': len(inter),\n",
    "            'coverage_rate_next': coverage,\n",
    "            'low_overlap_flag': (not np.isnan(coverage)) and (coverage < 0.7)\n",
    "        })\n",
    "    return pd.DataFrame(summary_rows)\n",
    "\n",
    "prev_counts = previous_df.groupby('LLMScore').agg(\n",
    "    rows=('TestCaseId', 'size'),\n",
    "    unique_cases=('TestCaseId', pd.Series.nunique)\n",
    ").reset_index()\n",
    "next_counts = next_df.groupby('LLMScore').agg(\n",
    "    rows=('TestCaseId', 'size'),\n",
    "    unique_cases=('TestCaseId', pd.Series.nunique)\n",
    ").reset_index()\n",
    "print('Previous snapshot:')\n",
    "display(prev_counts)\n",
    "print('Next snapshot:')\n",
    "display(next_counts)\n",
    "coverage_df = coverage_snapshot(previous_df, next_df)\n",
    "print('Coverage overview:')\n",
    "display(coverage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db6345ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TestCaseId</th>\n",
       "      <th>LLMScore</th>\n",
       "      <th>X_baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Efficiency</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Relevance</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>3.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Efficiency</td>\n",
       "      <td>2.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TestCaseId    LLMScore  X_baseline\n",
       "0           1    Accuracy    4.000000\n",
       "1           1  Efficiency    4.000000\n",
       "2           1   Relevance    4.000000\n",
       "3           2    Accuracy    3.166667\n",
       "4           2  Efficiency    2.666667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline statistics per metric:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LLMScore</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>n_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>2.183908</td>\n",
       "      <td>1.053797</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Efficiency</td>\n",
       "      <td>3.310577</td>\n",
       "      <td>0.752408</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Relevance</td>\n",
       "      <td>3.438697</td>\n",
       "      <td>0.680627</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     LLMScore      mean       std  n_cases\n",
       "0    Accuracy  2.183908  1.053797       87\n",
       "1  Efficiency  3.310577  0.752408       87\n",
       "2   Relevance  3.438697  0.680627       87"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 4 — Build CUPED Baseline (X) from Previous\n",
    "X_baseline = (\n",
    "    previous_df\n",
    "    .groupby(['TestCaseId', 'LLMScore'], as_index=False)['Value']\n",
    "    .mean()\n",
    "    .rename(columns={'Value': 'X_baseline'})\n",
    ")\n",
    "metric_baseline_stats = (\n",
    "    X_baseline.groupby('LLMScore')['X_baseline']\n",
    "    .agg(['mean', 'std', 'count'])\n",
    "    .rename(columns={'count': 'n_cases'})\n",
    "    .reset_index()\n",
    ")\n",
    "print('Baseline preview:')\n",
    "display(X_baseline.head())\n",
    "print('Baseline statistics per metric:')\n",
    "display(metric_baseline_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dda474a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta estimates per metric:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aadkannan\\AppData\\Local\\Temp\\ipykernel_30416\\3313215851.py:22: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  ols_slope = model.params[1]\n",
      "C:\\Users\\aadkannan\\AppData\\Local\\Temp\\ipykernel_30416\\3313215851.py:22: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  ols_slope = model.params[1]\n",
      "C:\\Users\\aadkannan\\AppData\\Local\\Temp\\ipykernel_30416\\3313215851.py:22: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  ols_slope = model.params[1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LLMScore</th>\n",
       "      <th>theta</th>\n",
       "      <th>corr_XY</th>\n",
       "      <th>var_X</th>\n",
       "      <th>N_rows_prev</th>\n",
       "      <th>OLS_slope</th>\n",
       "      <th>variance_or_corr_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.699721</td>\n",
       "      <td>1.097723</td>\n",
       "      <td>522</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Efficiency</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809969</td>\n",
       "      <td>0.559610</td>\n",
       "      <td>522</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Relevance</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.790490</td>\n",
       "      <td>0.457928</td>\n",
       "      <td>522</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     LLMScore  theta   corr_XY     var_X  N_rows_prev  OLS_slope  \\\n",
       "0    Accuracy    1.0  0.699721  1.097723          522        1.0   \n",
       "1  Efficiency    1.0  0.809969  0.559610          522        1.0   \n",
       "2   Relevance    1.0  0.790490  0.457928          522        1.0   \n",
       "\n",
       "   variance_or_corr_flag  \n",
       "0                  False  \n",
       "1                  False  \n",
       "2                  False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 5 — Estimate θ per Metric (CUPED coefficient)\n",
    "prev_with_baseline = previous_df.merge(X_baseline, on=['TestCaseId', 'LLMScore'], how='left')\n",
    "theta_rows = []\n",
    "for metric, group in prev_with_baseline.groupby('LLMScore'):\n",
    "    x = group['X_baseline']\n",
    "    y = group['Value']\n",
    "    x_centered = x - x.mean()\n",
    "    y_centered = y - y.mean()\n",
    "    var_x = x_centered.var(ddof=0)\n",
    "    if np.isclose(var_x, 0):\n",
    "        theta = 0.0\n",
    "        warn = True\n",
    "    else:\n",
    "        cov = np.mean(x_centered * y_centered)\n",
    "        theta = cov / var_x\n",
    "        warn = False\n",
    "    corr_xy = np.corrcoef(x, y)[0, 1] if len(group) > 1 else np.nan\n",
    "    ols_slope = np.nan\n",
    "    if OLS is not None and not np.isclose(var_x, 0):\n",
    "        X = add_constant(x_centered)\n",
    "        model = OLS(y_centered, X).fit()\n",
    "        ols_slope = model.params[1]\n",
    "    theta_rows.append({\n",
    "        'LLMScore': metric,\n",
    "        'theta': theta,\n",
    "        'corr_XY': corr_xy,\n",
    "        'var_X': var_x,\n",
    "        'N_rows_prev': len(group),\n",
    "        'OLS_slope': ols_slope,\n",
    "        'variance_or_corr_flag': warn or (not np.isnan(corr_xy) and abs(corr_xy) < 0.05)\n",
    "    })\n",
    "\n",
    "theta_df = pd.DataFrame(theta_rows)\n",
    "print('Theta estimates per metric:')\n",
    "display(theta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "223a0cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance diagnostics per metric:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LLMScore</th>\n",
       "      <th>var_raw</th>\n",
       "      <th>var_cuped</th>\n",
       "      <th>variance_reduction_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>2.279321</td>\n",
       "      <td>1.118709</td>\n",
       "      <td>50.919204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Efficiency</td>\n",
       "      <td>0.845569</td>\n",
       "      <td>0.294570</td>\n",
       "      <td>65.163147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Relevance</td>\n",
       "      <td>0.737186</td>\n",
       "      <td>0.283262</td>\n",
       "      <td>61.575258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     LLMScore   var_raw  var_cuped  variance_reduction_pct\n",
       "0    Accuracy  2.279321   1.118709               50.919204\n",
       "1  Efficiency  0.845569   0.294570               65.163147\n",
       "2   Relevance  0.737186   0.283262               61.575258"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 6 — Apply CUPED to Previous & Next\n",
    "mean_X_baseline = X_baseline.groupby('LLMScore')['X_baseline'].mean().rename('mean_X_baseline')\n",
    "X_with_mean = X_baseline.merge(mean_X_baseline, on='LLMScore', how='left')\n",
    "\n",
    "def apply_cuped(df: pd.DataFrame, label: str) -> pd.DataFrame:\n",
    "    merged = df.merge(X_with_mean, on=['TestCaseId', 'LLMScore'], how='left')\n",
    "    merged = merged.merge(theta_df[['LLMScore', 'theta']], on='LLMScore', how='left')\n",
    "    merged['X_baseline'] = merged['X_baseline'].fillna(merged['mean_X_baseline'])\n",
    "    merged['theta'] = merged['theta'].fillna(0.0)\n",
    "    merged['Value_CUPED'] = merged['Value'] - merged['theta'] * (merged['X_baseline'] - merged['mean_X_baseline'])\n",
    "    merged['Period'] = label\n",
    "    return merged\n",
    "\n",
    "previous_cuped = apply_cuped(previous_df, 'Previous')\n",
    "next_cuped = apply_cuped(next_df, 'Next')\n",
    "\n",
    "variance_rows = []\n",
    "for metric, group in pd.concat([previous_cuped, next_cuped]).groupby('LLMScore'):\n",
    "    var_raw = group['Value'].var(ddof=0)\n",
    "    var_cuped = group['Value_CUPED'].var(ddof=0)\n",
    "    reduction = 1 - (var_cuped / var_raw) if var_raw > 0 else np.nan\n",
    "    variance_rows.append({\n",
    "        'LLMScore': metric,\n",
    "        'var_raw': var_raw,\n",
    "        'var_cuped': var_cuped,\n",
    "        'variance_reduction_pct': reduction * 100 if not np.isnan(reduction) else np.nan\n",
    "    })\n",
    "variance_df = pd.DataFrame(variance_rows)\n",
    "print('Variance diagnostics per metric:')\n",
    "display(variance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78eca4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-case CUPED summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Period</th>\n",
       "      <th>TestCaseId</th>\n",
       "      <th>LLMScore</th>\n",
       "      <th>Next_CUPED_Mean</th>\n",
       "      <th>Prev_CUPED_Mean</th>\n",
       "      <th>Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>2.580518</td>\n",
       "      <td>2.183908</td>\n",
       "      <td>0.396610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Efficiency</td>\n",
       "      <td>3.400792</td>\n",
       "      <td>3.310577</td>\n",
       "      <td>0.090215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Relevance</td>\n",
       "      <td>3.442425</td>\n",
       "      <td>3.438697</td>\n",
       "      <td>0.003727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>2.218469</td>\n",
       "      <td>2.183908</td>\n",
       "      <td>0.034561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Efficiency</td>\n",
       "      <td>2.682404</td>\n",
       "      <td>3.310577</td>\n",
       "      <td>-0.628174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Period  TestCaseId    LLMScore  Next_CUPED_Mean  Prev_CUPED_Mean      Diff\n",
       "0                1    Accuracy         2.580518         2.183908  0.396610\n",
       "1                1  Efficiency         3.400792         3.310577  0.090215\n",
       "2                1   Relevance         3.442425         3.438697  0.003727\n",
       "3                2    Accuracy         2.218469         2.183908  0.034561\n",
       "4                2  Efficiency         2.682404         3.310577 -0.628174"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-metric CUPED summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LLMScore</th>\n",
       "      <th>mean_diff</th>\n",
       "      <th>cohens_d</th>\n",
       "      <th>percent_change</th>\n",
       "      <th>share_improved</th>\n",
       "      <th>n_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>-0.203914</td>\n",
       "      <td>-0.210296</td>\n",
       "      <td>-0.093371</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Efficiency</td>\n",
       "      <td>0.144672</td>\n",
       "      <td>0.270047</td>\n",
       "      <td>0.043700</td>\n",
       "      <td>0.781609</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Relevance</td>\n",
       "      <td>0.188412</td>\n",
       "      <td>0.340322</td>\n",
       "      <td>0.054792</td>\n",
       "      <td>0.770115</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     LLMScore  mean_diff  cohens_d  percent_change  share_improved  n_cases\n",
       "0    Accuracy  -0.203914 -0.210296       -0.093371        0.413793       87\n",
       "1  Efficiency   0.144672  0.270047        0.043700        0.781609       87\n",
       "2   Relevance   0.188412  0.340322        0.054792        0.770115       87"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 7 — Per-Case & Per-Metric Comparisons (CUPED space)\n",
    "all_cuped = pd.concat([previous_cuped, next_cuped], ignore_index=True)\n",
    "per_case = (\n",
    "    all_cuped\n",
    "    .groupby(['TestCaseId', 'LLMScore', 'Period'])['Value_CUPED']\n",
    "    .mean()\n",
    "    .unstack('Period')\n",
    "    .rename(columns={'Previous': 'Prev_CUPED_Mean', 'Next': 'Next_CUPED_Mean'})\n",
    ")\n",
    "per_case['Diff'] = per_case['Next_CUPED_Mean'] - per_case['Prev_CUPED_Mean']\n",
    "per_case = per_case.reset_index()\n",
    "print('Per-case CUPED summary:')\n",
    "display(per_case.head())\n",
    "\n",
    "def cohen_d(diff_values: np.ndarray) -> float:\n",
    "    diff_values = diff_values[~np.isnan(diff_values)]\n",
    "    if diff_values.size == 0:\n",
    "        return np.nan\n",
    "    mean_diff = diff_values.mean()\n",
    "    std_diff = diff_values.std(ddof=1)\n",
    "    return mean_diff / std_diff if std_diff > 0 else np.nan\n",
    "\n",
    "metric_summary_rows = []\n",
    "for metric, group in per_case.groupby('LLMScore'):\n",
    "    diffs = group['Diff'].dropna().values\n",
    "    prev_mean = group['Prev_CUPED_Mean'].mean()\n",
    "    next_mean = group['Next_CUPED_Mean'].mean()\n",
    "    percent_change = (next_mean / prev_mean - 1) if prev_mean != 0 else np.nan\n",
    "    share_improved = np.mean(diffs > 0) if len(diffs) else np.nan\n",
    "    metric_summary_rows.append({\n",
    "        'LLMScore': metric,\n",
    "        'mean_diff': np.mean(diffs) if len(diffs) else np.nan,\n",
    "        'cohens_d': cohen_d(diffs),\n",
    "        'percent_change': percent_change,\n",
    "        'share_improved': share_improved,\n",
    "        'n_cases': len(diffs)\n",
    "    })\n",
    "metric_summary_df = pd.DataFrame(metric_summary_rows)\n",
    "print('Per-metric CUPED summary:')\n",
    "display(metric_summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2efdf642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap summary per metric:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LLMScore</th>\n",
       "      <th>mean_diff</th>\n",
       "      <th>SE</th>\n",
       "      <th>CI_lo_95</th>\n",
       "      <th>CI_hi_95</th>\n",
       "      <th>N_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>-0.203914</td>\n",
       "      <td>0.102973</td>\n",
       "      <td>-0.407062</td>\n",
       "      <td>-0.001659</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Efficiency</td>\n",
       "      <td>0.144672</td>\n",
       "      <td>0.056575</td>\n",
       "      <td>0.031900</td>\n",
       "      <td>0.253119</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Relevance</td>\n",
       "      <td>0.188412</td>\n",
       "      <td>0.058094</td>\n",
       "      <td>0.074562</td>\n",
       "      <td>0.303024</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     LLMScore  mean_diff        SE  CI_lo_95  CI_hi_95  N_cases\n",
       "0    Accuracy  -0.203914  0.102973 -0.407062 -0.001659       87\n",
       "1  Efficiency   0.144672  0.056575  0.031900  0.253119       87\n",
       "2   Relevance   0.188412  0.058094  0.074562  0.303024       87"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall pooled bootstrap mean diff (descriptive):\n",
      "count    10000.000000\n",
      "mean         0.043513\n",
      "std          0.045051\n",
      "min         -0.150922\n",
      "2.5%        -0.044616\n",
      "50%          0.043646\n",
      "97.5%        0.130498\n",
      "max          0.198156\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 — Bootstrap Inference (clustered by TestCaseID)\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng(RANDOM_SEED)\n",
    "\n",
    "def cluster_bootstrap_mean(diffs: pd.Series, ids: pd.Series, n_boot: int) -> np.ndarray:\n",
    "    unique_ids = ids.unique()\n",
    "    diff_map = pd.Series(diffs.values, index=ids.values).to_dict()\n",
    "    boot_means = np.empty(n_boot)\n",
    "    for i in range(n_boot):\n",
    "        sampled_ids = rng.choice(unique_ids, size=len(unique_ids), replace=True)\n",
    "        sampled_values = np.array([diff_map[idx] for idx in sampled_ids])\n",
    "        boot_means[i] = sampled_values.mean()\n",
    "    return boot_means\n",
    "\n",
    "bootstrap_rows = []\n",
    "bootstrap_results: Dict[str, np.ndarray] = {}\n",
    "for metric, group in per_case.groupby('LLMScore'):\n",
    "    mask = group['Diff'].notna()\n",
    "    if not mask.any():\n",
    "        continue\n",
    "    diffs = group.loc[mask, 'Diff']\n",
    "    ids = group.loc[mask, 'TestCaseId']\n",
    "    boot_means = cluster_bootstrap_mean(diffs, ids, N_BOOT)\n",
    "    bootstrap_results[metric] = boot_means\n",
    "    se = boot_means.std(ddof=1)\n",
    "    ci_lo, ci_hi = np.percentile(boot_means, [2.5, 97.5])\n",
    "    bootstrap_rows.append({\n",
    "        'LLMScore': metric,\n",
    "        'mean_diff': diffs.mean(),\n",
    "        'SE': se,\n",
    "        'CI_lo_95': ci_lo,\n",
    "        'CI_hi_95': ci_hi,\n",
    "        'N_cases': len(diffs)\n",
    "    })\n",
    "bootstrap_df = pd.DataFrame(bootstrap_rows)\n",
    "print('Bootstrap summary per metric:')\n",
    "display(bootstrap_df)\n",
    "\n",
    "pooled_diffs = per_case[['TestCaseId', 'LLMScore', 'Diff']].dropna()\n",
    "if not pooled_diffs.empty:\n",
    "    pooled_boot_means = []\n",
    "    unique_pairs = pooled_diffs[['TestCaseId', 'LLMScore']].apply(lambda x: tuple(x), axis=1).values\n",
    "    diff_map = {key: val for key, val in zip(unique_pairs, pooled_diffs['Diff'].values)}\n",
    "    for _ in range(N_BOOT):\n",
    "        sampled_pairs = rng.choice(unique_pairs, size=len(unique_pairs), replace=True)\n",
    "        pooled_boot_means.append(np.mean([diff_map[key] for key in sampled_pairs]))\n",
    "    print('Overall pooled bootstrap mean diff (descriptive):')\n",
    "    print(pd.Series(pooled_boot_means).describe(percentiles=[0.025, 0.5, 0.975]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "645f0543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot files saved:\n",
      "Efficiency cuped_kde outputs\\plots\\kde_cuped_Efficiency.png\n",
      "Efficiency bootstrap_kde outputs\\plots\\kde_bootstrap_Efficiency.png\n",
      "Relevance cuped_kde outputs\\plots\\kde_cuped_Relevance.png\n",
      "Relevance bootstrap_kde outputs\\plots\\kde_bootstrap_Relevance.png\n",
      "Accuracy cuped_kde outputs\\plots\\kde_cuped_Accuracy.png\n",
      "Accuracy bootstrap_kde outputs\\plots\\kde_bootstrap_Accuracy.png\n",
      "Bar chart path: outputs\\plots\\mean_diff_bar.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aadkannan\\AppData\\Local\\Temp\\ipykernel_30416\\1907439382.py:43: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  bar_plot = sns.barplot(data=bootstrap_df, x='LLMScore', y='mean_diff', palette='viridis', ax=ax)\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 — Visuals: Distribution & Effect Views\n",
    "plot_dir = Path('outputs/plots')\n",
    "plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def plot_metric_kdes(metric: str):\n",
    "    metric_data = all_cuped[all_cuped['LLMScore'] == metric]\n",
    "    plt.figure()\n",
    "    sns.kdeplot(data=metric_data, x='Value_CUPED', hue='Period', common_norm=False)\n",
    "    plt.title(f'CUPED Value Distribution — {metric}')\n",
    "    plt.xlabel('Value (CUPED)')\n",
    "    plt.ylabel('Density')\n",
    "    plt.tight_layout()\n",
    "    path = plot_dir / f\"kde_cuped_{metric}.png\"\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "    return path\n",
    "\n",
    "def plot_bootstrap_kde(metric: str, boot_means: np.ndarray):\n",
    "    plt.figure()\n",
    "    sns.kdeplot(boot_means, fill=True)\n",
    "    plt.axvline(0, color='black', linestyle='--', label='Zero')\n",
    "    plt.axvline(np.mean(boot_means), color='blue', linestyle='-', label='Mean')\n",
    "    ci_lo, ci_hi = np.percentile(boot_means, [2.5, 97.5])\n",
    "    plt.axvline(ci_lo, color='red', linestyle='--', label='95% CI')\n",
    "    plt.axvline(ci_hi, color='red', linestyle='--')\n",
    "    plt.title(f'Bootstrap Mean Diff Distribution — {metric}')\n",
    "    plt.xlabel('Bootstrap Mean Diff')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    path = plot_dir / f\"kde_bootstrap_{metric}.png\"\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "    return path\n",
    "\n",
    "metric_plot_paths = {}\n",
    "for metric in all_cuped['LLMScore'].unique():\n",
    "    metric_plot_paths[metric] = {'cuped_kde': plot_metric_kdes(metric)}\n",
    "    if metric in bootstrap_results:\n",
    "        metric_plot_paths[metric]['bootstrap_kde'] = plot_bootstrap_kde(metric, bootstrap_results[metric])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(max(8, 2 * max(1, len(bootstrap_df))), 6))\n",
    "bar_plot = sns.barplot(data=bootstrap_df, x='LLMScore', y='mean_diff', palette='viridis', ax=ax)\n",
    "positions = ax.get_xticks()\n",
    "ax.errorbar(positions, bootstrap_df['mean_diff'],\n",
    "            yerr=[bootstrap_df['mean_diff'] - bootstrap_df['CI_lo_95'],\n",
    "                  bootstrap_df['CI_hi_95'] - bootstrap_df['mean_diff']],\n",
    "            fmt='none', c='black', capsize=5)\n",
    "ax.axhline(0, color='black', linestyle='--')\n",
    "ax.set_title('Mean CUPED Difference per Metric with 95% CI')\n",
    "ax.set_xlabel('LLMScore')\n",
    "ax.set_ylabel('Mean Diff (Next - Previous)')\n",
    "fig.tight_layout()\n",
    "bar_plot_path = plot_dir / 'mean_diff_bar.png'\n",
    "fig.savefig(bar_plot_path)\n",
    "plt.close(fig)\n",
    "\n",
    "print('Plot files saved:')\n",
    "for metric, paths in metric_plot_paths.items():\n",
    "    for kind, path in paths.items():\n",
    "        print(metric, kind, path)\n",
    "print('Bar chart path:', bar_plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa537fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple comparison table (exploratory):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LLMScore</th>\n",
       "      <th>mean_diff</th>\n",
       "      <th>SE</th>\n",
       "      <th>CI_lo_95</th>\n",
       "      <th>CI_hi_95</th>\n",
       "      <th>N_cases</th>\n",
       "      <th>p_value_ttest</th>\n",
       "      <th>q_value_bh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>-0.203914</td>\n",
       "      <td>0.102973</td>\n",
       "      <td>-0.407062</td>\n",
       "      <td>-0.001659</td>\n",
       "      <td>87</td>\n",
       "      <td>0.053055</td>\n",
       "      <td>0.053055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Efficiency</td>\n",
       "      <td>0.144672</td>\n",
       "      <td>0.056575</td>\n",
       "      <td>0.031900</td>\n",
       "      <td>0.253119</td>\n",
       "      <td>87</td>\n",
       "      <td>0.013625</td>\n",
       "      <td>0.020437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Relevance</td>\n",
       "      <td>0.188412</td>\n",
       "      <td>0.058094</td>\n",
       "      <td>0.074562</td>\n",
       "      <td>0.303024</td>\n",
       "      <td>87</td>\n",
       "      <td>0.002084</td>\n",
       "      <td>0.006253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     LLMScore  mean_diff        SE  CI_lo_95  CI_hi_95  N_cases  \\\n",
       "0    Accuracy  -0.203914  0.102973 -0.407062 -0.001659       87   \n",
       "1  Efficiency   0.144672  0.056575  0.031900  0.253119       87   \n",
       "2   Relevance   0.188412  0.058094  0.074562  0.303024       87   \n",
       "\n",
       "   p_value_ttest  q_value_bh  \n",
       "0       0.053055    0.053055  \n",
       "1       0.013625    0.020437  \n",
       "2       0.002084    0.006253  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 10 — Multiple-Comparison Context (optional)\n",
    "try:\n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "    from scipy.stats import t\n",
    "except ImportError:\n",
    "    multipletests = None\n",
    "    t = None\n",
    "\n",
    "if multipletests is not None and t is not None and not per_case.empty:\n",
    "    p_values = []\n",
    "    for metric, group in per_case.groupby('LLMScore'):\n",
    "        diffs = group['Diff'].dropna()\n",
    "        if len(diffs) < 2:\n",
    "            p_values.append(np.nan)\n",
    "            continue\n",
    "        mean_diff = diffs.mean()\n",
    "        std_diff = diffs.std(ddof=1)\n",
    "        if std_diff == 0:\n",
    "            p_values.append(np.nan)\n",
    "            continue\n",
    "        t_stat = mean_diff / (std_diff / np.sqrt(len(diffs)))\n",
    "        p_val = 2 * (1 - t.cdf(abs(t_stat), df=len(diffs) - 1))\n",
    "        p_values.append(p_val)\n",
    "    temp_df = bootstrap_df.copy()\n",
    "    temp_df['p_value_ttest'] = p_values\n",
    "    valid_mask = temp_df['p_value_ttest'].notna()\n",
    "    if valid_mask.any():\n",
    "        _, q_values, _, _ = multipletests(temp_df.loc[valid_mask, 'p_value_ttest'], method='fdr_bh')\n",
    "        temp_df.loc[valid_mask, 'q_value_bh'] = q_values\n",
    "    print('Multiple comparison table (exploratory):')\n",
    "    display(temp_df)\n",
    "else:\n",
    "    print('Multiple comparison analysis skipped (dependencies unavailable or insufficient data).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ee8ef82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity analyses overview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LLMScore</th>\n",
       "      <th>mean_diff</th>\n",
       "      <th>n_cases</th>\n",
       "      <th>mean_diff_intersection</th>\n",
       "      <th>n_cases_intersection</th>\n",
       "      <th>mean_diff_winsor</th>\n",
       "      <th>median_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>-0.203914</td>\n",
       "      <td>87</td>\n",
       "      <td>-0.203914</td>\n",
       "      <td>87</td>\n",
       "      <td>-0.189034</td>\n",
       "      <td>-0.149064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Efficiency</td>\n",
       "      <td>0.144672</td>\n",
       "      <td>87</td>\n",
       "      <td>0.144672</td>\n",
       "      <td>87</td>\n",
       "      <td>0.158090</td>\n",
       "      <td>0.178276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Relevance</td>\n",
       "      <td>0.188412</td>\n",
       "      <td>87</td>\n",
       "      <td>0.188412</td>\n",
       "      <td>87</td>\n",
       "      <td>0.191468</td>\n",
       "      <td>0.166726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     LLMScore  mean_diff  n_cases  mean_diff_intersection  \\\n",
       "0    Accuracy  -0.203914       87               -0.203914   \n",
       "1  Efficiency   0.144672       87                0.144672   \n",
       "2   Relevance   0.188412       87                0.188412   \n",
       "\n",
       "   n_cases_intersection  mean_diff_winsor  median_diff  \n",
       "0                    87         -0.189034    -0.149064  \n",
       "1                    87          0.158090     0.178276  \n",
       "2                    87          0.191468     0.166726  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 11 — Sensitivity Analyses\n",
    "intersection_cases = set(previous_df['TestCaseId']).intersection(set(next_df['TestCaseId']))\n",
    "per_case_intersection = per_case[per_case['TestCaseId'].isin(intersection_cases)].copy()\n",
    "intersection_summary = (\n",
    "    per_case_intersection.groupby('LLMScore')['Diff']\n",
    "    .agg(mean_diff_intersection='mean', n_cases_intersection='count')\n",
    ")\n",
    "\n",
    "def winsorize(series: pd.Series, lower=0.025, upper=0.975) -> pd.Series:\n",
    "    if series.isna().all():\n",
    "        return series\n",
    "    lower_bound = series.quantile(lower)\n",
    "    upper_bound = series.quantile(upper)\n",
    "    return series.clip(lower_bound, upper_bound)\n",
    "\n",
    "per_case_winsor = per_case.copy()\n",
    "per_case_winsor['Diff_winsor'] = per_case.groupby('LLMScore')['Diff'].transform(winsorize)\n",
    "winsor_summary = (\n",
    "    per_case_winsor.groupby('LLMScore')['Diff_winsor']\n",
    "    .agg(mean_diff_winsor='mean')\n",
    ")\n",
    "median_summary = (\n",
    "    per_case.groupby('LLMScore')['Diff']\n",
    "    .agg(median_diff='median')\n",
    ")\n",
    "\n",
    "sensitivity_df = (\n",
    "    metric_summary_df[['LLMScore', 'mean_diff', 'n_cases']]\n",
    "    .merge(intersection_summary, on='LLMScore', how='left')\n",
    "    .merge(winsor_summary, on='LLMScore', how='left')\n",
    "    .merge(median_summary, on='LLMScore', how='left')\n",
    ")\n",
    "print('Sensitivity analyses overview:')\n",
    "display(sensitivity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7093809f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exports saved:\n",
      "outputs\\per_case_cuped_diff.csv\n",
      "outputs\\metric_cuped_summary_raw.csv\n",
      "outputs\\metric_cuped_summary_bootstrap.csv\n",
      "outputs\\theta_table.csv\n",
      "outputs\\variance_reduction_by_metric.csv\n",
      "outputs\\coverage_by_metric.csv\n",
      "Plots directory: outputs\\plots\n"
     ]
    }
   ],
   "source": [
    "# Cell 12 — Final Summary Tables & Exports\n",
    "output_dir = Path('outputs')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "per_case_path = output_dir / 'per_case_cuped_diff.csv'\n",
    "metric_summary_path = output_dir / 'metric_cuped_summary_raw.csv'\n",
    "bootstrap_path = output_dir / 'metric_cuped_summary_bootstrap.csv'\n",
    "theta_path = output_dir / 'theta_table.csv'\n",
    "variance_path = output_dir / 'variance_reduction_by_metric.csv'\n",
    "coverage_path = output_dir / 'coverage_by_metric.csv'\n",
    "\n",
    "per_case.to_csv(per_case_path, index=False)\n",
    "metric_summary_df.to_csv(metric_summary_path, index=False)\n",
    "bootstrap_df.to_csv(bootstrap_path, index=False)\n",
    "theta_df.to_csv(theta_path, index=False)\n",
    "variance_df.to_csv(variance_path, index=False)\n",
    "coverage_df.to_csv(coverage_path, index=False)\n",
    "\n",
    "print('Exports saved:')\n",
    "for path in [per_case_path, metric_summary_path, bootstrap_path, theta_path, variance_path, coverage_path]:\n",
    "    print(path)\n",
    "print('Plots directory:', plot_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2284b998",
   "metadata": {},
   "source": [
    "## Interpretation Guide\n",
    "\n",
    "* **Primary metric:** The CUPED-adjusted mean difference (Next − Previous) summarizes the directional effect for each `LLMScore`.\n",
    "* **Uncertainty:** 95% bootstrap confidence intervals that exclude zero indicate stronger evidence that the effect is non-zero.\n",
    "* **Variance reduction:** Compare raw vs CUPED variance; larger reductions imply a more informative baseline (`corr(X, Y)` close to ±1).\n",
    "* **Coverage:** Low overlap between previous and next test cases weakens comparability—consult the intersection-only sensitivity table.\n",
    "* **Multiple metrics:** Use the exploratory q-values to control for multiple comparisons before declaring broad improvements.\n",
    "* **Practical impact:** Review Cohen’s d, percent change, and share of improved cases to gauge real-world significance.\n",
    "* **Diagnostics:** Negative variance reduction or negligible correlations highlight metrics where CUPED may not help."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuped-env)",
   "language": "python",
   "name": "cuped-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
